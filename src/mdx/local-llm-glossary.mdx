---
title: 'Local LLM Glossary'
type: 'reference'
stage: 'atom'
topics: ['Generative AI', 'LLM', 'LocalLlama']
createdAt: '2024-01-15'
updatedAt: '2024-01-16'
---

Before diving into a local LLM community, here are some key terms to familiarize yourself with as you get started. If you wish to read this note from beginning to end, I recommend checking out [the original version](https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide) authored by [Omar Sanseviero](https://twitter.com/osanseviero), as it is structured with progressive disclosure.

## A

**adapters**: a popular library to do PEFT.

**AIF (AI Feedback)**: An alternative to human feedback.

**Alpaca**: a dataset of 52,000 instructions generatd with OpenAI APIs. It kicked off a big wave of people using OpenAI to generate synthetic data for instruct-tuning. It costed about $500 to generate.

**Auto-regressive**: a type of model that generates text one token at a time. It is auto-regressive because it uses its own predictions to generate the next token. For example, the model might receive as input "Today's weather" and generate the next token, "is". It will then use "Today's weather is" as input and generate the next token, "sunny". It will then use "Today's weather is sunny" as input and generate the next token, "and". And so on.

**Averaging**: the most basic merging technique. Pick two models, average their weights. Somehow it kind of works.

**AWQ**: a popular quantization technique.

**axolotl**: a cute animal that is also a high-level tool to streamline fine-tuning, including support for things such as QLoRA.

## B

**Bagel**: a process which mixes a bunch of supervised fine-tuning and preference data. It uses different prompt formats, making the model more versatile to all kinds of prompts.

**Base vs. conversational**: a pre-trained model is not specifically trained to "behave" in a conversational manner. If you try to use a base model (e.g. GPT-3, Mistral, Llama) directly to do conversations, it won't work as well as the fine-tuned conversational variant (ChatGPT, Mistral Instruct, Llama Chat). When looking at benchmarks, you want to compare base models with base models and conversational models with conversational models.

**Benchmark**: a benchmark is a test that you run to compare different models. For example, you can run a benchmark to compare the performance of different models on a specific task.

**BigCode**: an open scientific collaboration working in code-related models and datasets.

**Big Code Models Leaderboard**: a [leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) to compare code models in the HumanEval dataset.

## C

**Chatbot Arena**: a popular [crowd-sourced open benchmark](https://lmsys.org/blog/2023-05-03-arena/) of human preferences. It's good to compare conversational models.

**ChatGPT**: RLHF-finetuned GPT-3 model that is very good at conversations.

**ChatUI**: an open-source UI to use open-source models.

**Code Llama**: the best base code model. It's based on Llama 2.

**Code Models**: LLMs that are specifically pre-trained for code.

**Cognitive Computations**: a community (led by [Eric Hartford](https://twitter.com/erhartford)) that is fine-tuning a bunch of models.

**Context length**: the number of tokens that the model can use at a time. The higher the context length, the more memory the model needs to train and the slower it is to run. E.g. Llama 2 can manage up to 4096 tokens.

**Conversational models**: the LLM Leaderboard should be mostly to compare base models, not as much for conversational models. It still provides some useful signal about the conversational models, but this should not be the final way to evaluate them.

## D

**DPO**: a type of training which removes the need for a reward model. It simplifies significantly the RLHF-pipeline.

**DPO Overfits**: although DPO shows overfitting behaviors after one behavior, it does not harm downstream performance on chat evaluations. Did your ML teachers lie to us when they said overfitting was bad?

## E

**EXL2**: a quantization format used by a library called exllamav2 (among many others).

## F

**Few-shot**: a type of prompt that is used to generate text with fine-tuning. We provide a couple of examples to the model. This can improve the quality a lot.

**Fine-tuning**: training a model on a small (labeled) dataset to learn a specific task. This is done after pre-training. Imagine you have a few dollars, as a good fellow GPU-Poor. Rather than training a model from scratch, you pick a pre-trained (base) model and fine-tune it. You usually pick a small dataset of few hundreds-thousands of samples. You then pass it to the model and train on it. This is called fine-tuning. The idea is to end with a model that has a strong understanding of a specific task. For example, you can fine-tune a model with your tweets to make it generate tweets like you (but please don't). You can fine-tune many models in your gaming laptop. Examples of fine-tuned models are ChatGPT, Vicuna, and Mistral Instruct.

**Flash Attention**: an approximate attention algorithm which provides a huge speedup.

**Flash Attention 2**: an upgrade to the flash attention algorithm that provides even more speedup.

**Frankenmerge**: it allows to concatenate layers from different LLMs, allowing you to do crazy things.

## G

**Georgi Gerganov**: the creator of llama.cpp and ggml.

**ggml**: tensor library in ML, allowing projects such as llama.cpp and whisper.cpp (not the same as GGML, the file format).

**GGUF**: a format introduced by llama.cpp to store models. It replaces the old file format, GGML.

**Goliath-120B**: a frankenmerge that combines two Llama 70B models to achieve a 120B model.

**GPT**: a type of transformer that is trained to predict the next token in a sentence. GPT-3 is an example of a GPT model.

**GPT-4**: a kind of good model, but we don't know what it is. The rumors say it's a MoE.

**GPTQ**: a popular quantization technique.

**Guanaco (model)**: a LLaMA fine-tune using QLoRA tuning.

## H

**Hallucination**: when a model generates responses that may be coherent but are not actually accurate, leading to the creation of misinformation or imaginary scenarios.

**Hugging Face**: a platform to find and share open-acces models, datasets, and demos. It's also a company that has built different OS libraries.

**HumanEval**: a very small dataset of 164 Python programming problems. It is translated to 18 programming languages in MultiPL-E.

## I

**Instruct-tuning**: a type of fine-tuning that uses instructions to generate text ending in more controlled behavor in generating responses or performing tasks.

**IPO**: a change in the DPO objective which is simpler and less prone to overfitting.

## J

n/a

## K

**KTO**: while PPO, DPO, and IPO require pairs of accepted vs rejected generations, KTO just needs a binary label (accepted or rejected), hence allowing to scale to much more data.

## L

**LASER**: a technique that reduces the size of the model and increases its performance by reducing the rank of specific matrices. It requires no additional training.

**LIMA**: a model that demonstrates strong performance with very few examples. It demonstrates that adding more data does not always correlate with better quality.

**LLaMA**: a pre-trained model trained by Meta, shared with some groups in a private access, and then leaked. It led to an explosion of cool projects.

**Llama 2**: an open-access pre-trained model released by Meta. It led to another explosion of very cool projects, and this one was not leaked. The license is not technically open-source but it's still quite open and permissive, even for commercial use cases.

**llama.cpp**: a tool to use Llama-like models in C++.

**LlaVA**: a multimodal model that can receive images and text as input and generate text responses.

**LLM (Large Language Model)**: Usually a transformer-based model with a lot of parameters, numbering in the billions or even trillions.

**LM Studio**: a nice advanced app that runs models on your laptop, entirely offline.

**LocalLlama**: a [Reddit community](https://www.reddit.com/r/LocalLLaMA) of practitioners, researchers, and hackers doing all kinds of crazy things with ML models.

**Local LLMs**: if we have models small enough, we can run them in our computers or even our phones.

**Local LLM tools**: if you don't know how to code, these tools that can be useful.

**LoRA**: one of the most popular PEFT techniques. It adds low-rank "update matrices". The base model is frozen and only the update matrices are trained. This can be used for image classification, teaching Stable Diffusion the concept of your pet, or LLM fine-tuning.

## M

**Mergekit**: an open-source tool to quickly merge models.

**Mistral 7B**: a pre-trained model trained by Mistral. Released via torrent.

**Mistral 7B Instruct**: a fine-tuned version of Mistral 7B.

**MLX**: a new framework for Apple devices that allows easy inference and fine-tuning of models.

**Mixtral**: a MoE model released by Mistral. It has 47B parameters but only 12B parameters are used at a time, making it very efficient.

**Mixture-of-Experts (MoE)**: a model architecture in which some of the (dense) layers are replaced with a set of experts. Each expert is a small neural network. There is a small network, router, that decides which expert to use for each token (read more [here](https://huggingface.co/blog/moe)). Clarifications:

- A MoE is not an ensemble.
- If we say a MoE has 8 experts, it means each replaced dense layer is replaced with 8 experts. If there were 3 replaced layers, then there are 24 experts in total.
- We can activate multiple experts at the same time. For a given sentence, "hello world", "hello might be sent to experts 1 and 2 while "world" to 2 and 4.
- The experts in a MoE do not specialize in a task. They are all trained on the same task, they just get different tokens. Sometimes they do specialize in certain types of tokens, as shown in Table 13 of [ST-MoE paper](https://arxiv.org/abs/2202.08906).

**Model Merging**: a technique that allows us to combine multiple models of the same architecture into a single model. Read more [here](https://huggingface.co/blog/mlabonne/merge-models).

**MoE Merging**: (not 100% about this one) experimental branch in `mergekit` that allows building a MoE-like model combining different models. You specify which models and which types of prompts you want each expert to handle, hence ending with expert task-specialization.

**MT-Bench**: a multi-turn benchmark of 160 questions across eight domains. Each response is evaluated by GPT-4. (This presents limitations: what happens if the model is better than GPT-4?)

**Multimodal**: a single model that can handle multiple modalities. For example, a model that can generate text and images at the same time. Or a model that can generate text and audio at the same time. Or a model that can generate text, images, and audio at the same time. Or a model that can generate text, images, audio, video, smells, tastes, feelings, thoughts, dreams, memories, consciousness, souls, universes, gods, multiverses, and omniverses at the same time. (thanks ChatGPT for your hallucination)

## N

**Notus**: a trained variation of Zephyr but with better filtered and fixed data. It does better.

**Nous Research**: an open-source Discord community turned company that releases bunch of cool models.

**Number of parameters**: notice the "13B" in models such as LLaMA-13B. That's the number of parameters in a model. Each parameter is a number (with certain precision), and is part of the model. The parameters are learned during pre-training and fine-tuning to minimize the error.

## O

**ollama**: an open-source tool to run LLMs locally. There are multiple web/desktop apps and terminal integrations on top of it.

**Oobabooga**: a simple web app that allows you to use models without coding. It's very easy to use.

**OpenAI**: a company that does closed source AI (just kidding, they open-sourced Whisper).

**Open LLM Leaderboard**: a [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) where you can find benchmark results for many open-access LLMs.

**Overfitting**: occurs in ML when a model learns the training data too well, capturing noise and specific patterns that do not generalize to new, unseen data, leading to poor performance on real-world tasks.

## P

**PEFT (Parameter-Efficient Fine-Tuning)**: a family of methods that allow fine-tuning models without modifying all the parameters. Usually, you freeze the model, add a small set of parameters, and modify it. This reduces the amount of compute required and allows you to achieve very good results.

**peft**: a popular OS library to do PEFT. It's used in other projects such as `trl`.

**Phi 2**: a pre-trained model by Microsoft. It only has 2.7B parameters, but it's quite good for its size. It was trained with very little data (textbooks) which shows the power of high-quality data.

**Phixtral**: a MoE merge of Phi 2 DPO and Dolphin 2 Phi 2.

**PPO**: a type of reinforcement learning algorithm that is used to train a model. It is used in RLHF.

**Pre-training**: training a model on a very large dataset (trillion of tokens) to learn the structure of language. Imagine you have millions of dollars, as a good GPU-Rich. You usually scrape big datasets from the internet and train your model on them. This is called pre-training. The idea is to end with a model that has a strong understanding of language. This does not require labeled data and is done before fine-tuning. Examples of pre-trained models are GPT-3, Llama 2, and Mistral.

**Prompt**: a few words that you give to the model to start generating text. For example, if you want to generate a poem, you can give the model the first line of the poem as a prompt. The model will then generate the rest of the poem.

## Q

**Quantization**: a technique that allows us to reduce the size of a model. It is done by reducing the precision of the model's weights. For example, we can reduce the precision from 32 bits to 8 bits. This reduces the size of the model by 4 times! The model will (sometimes) be less accurate but it will be much smaller. This allows us to run the model on smaller devices such as phones.

## R

**Reward Model**: a model that is used to generate rewards. For example, you can train a model to generate rewards for a game. The model will learn to generate rewards that are good for the game.

**RL (Reinforcement Learning)**: a type of machine learning that uses rewards to train a model. For example, you can train a model to play a game by giving it a reward when it wins and a punishment when it loses. The model will learn to win the game.

**RLHF (Reinforcement Learning with Human Feedback)**: a type of fine-tuning that uses reinforcement learning (RL) and human-generated feedback. Thanks to the introduction of human feedback, the end model ends up being very good for things such as conversations. It kicks off with a base model that generates bunch of conversations. Humans then rate the answers (preferences). The preferences are used to train a Reward Model that generates a score for a given text. Using Reinforcement Learning, the initial LM is trained to maximize the score generated by the Reward Model. Read more about it [here](https://huggingface.co/blog/rlhf).

**RoPE**: a technique that allows you to significantly expand the context lengths of a model.

## S

**SuperHot**: a technique that allows expanding the context length of RoPE-based models even more by doing some minimal additional training.

**SFT (Supervised Fine-Tuning)**: refining a pre-trained model for a specific task using a labeled dataset. A subset of fine-tuning.

## T

**TheBloke**: a bloke that quantizes models. As soon as a model is out, he quantizes it. See their [HF Profile](https://huggingface.co/TheBloke).

**The Stack**: a dataset of 6.4TB of permissible-licensed code data covering 358 programming languages.

**Tim Dettmers**: a researcher that has done a lot of work on PEFT and created QLoRA.

**TinyLlama**: a project to pre-train a 1.1B Llama model on 3 trillion tokens.

**Token**: models don't understand words. They understand numbers. When we receive a sequence of words, we convert them to numbers. Sometimes we split words into pieces, such as "tokenization" into "token" and "ization". This is needed because the model has a limited vocabulary. A token is the smallest unit of language that a model can understand.

**Transformer**: a type of neural network architecture that is very good at language tasks. It is the basis for most LLMs.

**transformers**: a Python library to access models shared by the community. It allows you to download pre-trained models and fine-tune them for your own needs.

**Tri Dao**: the author of Flash Attention algorithms and a legend in the ecosystem.

**trl**: a Python library that allows to train models with DPO, IPO, KTO, and more.

**TruthfulQA**: a not-great benchmark to measure a model's ability to generate truthful answers.

## U

**Uncensored models**: many models have some strong alignment that prevent doing things such as asking Llama to kill a Linux process. Training uncensored models aims to remove specific biases engrained in the decision-making process of fine-tuning a model. Read more [here](https://erichartford.com/uncensored-models).

**unsloth**: a higher-level library to do PEFT (using QLoRA).

## V

**Vicuna**: a cute animal that is also a fine-tuned model. It begins from LLaMA-13B and is fine-tuned on user conversations with ChatGPT.

## W

**Whisper**: the state-of-the-art speech-to-text open source model.

**WizardCoder**: a code model released by WizardLM. Its architecture is based on Llama.

**WizardLM**: a research team from Microsoft, but also a Discord community.

## X

n/a

## Y

n/a

## Z

**Zephyr**: a 7B Mistral-based model trained with DPO. It has similar capabilities to the Llama 2 Chat model of 70B parameters. It came out with a nice [handbook of recipes](https://github.com/huggingface/alignment-handbook/tree/main).

**Zero-shot**: a type of prompt that is used to generate text without fine-tuning. The model is not trained on any specific task. It is only trained on a large dataset of text. For example, you can give the model the first line of a poem and ask it to generate the rest of the poem. The model will do its best to generate a poem, even though it has never seen a poem before. When you use ChatGPT, you often do zero-shot generation.

---

**Resources**

- <Resource
    content={{
      text: 'The Llama Hitchiking Guide to Local LLMs',
      url: 'https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide',
    }}
    description="the main source of this note (Apache-2.0)"
    author={{
      text: 'Omar Sanseviero ',
      url: 'https://twitter.com/osanseviero',
    }}
  />
- <Resource
    content={{
      text: 'AI Glossary',
      url: 'https://a16z.com/ai-glossary',
    }}
    author={{ text: 'a16z' }}
  />
- <Resource
    content={{
      text: 'Glossary of artificial intelligence',
      url: 'https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence',
    }}
    author={{ text: 'Wikipedia' }}
  />
